{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comic Book Covers\n",
    "### Image & Metadata Exploratory Analysis\n",
    "\n",
    "This notebook contains an exploratory data analysis of comic book images and metadata scraped from https://comics.org. \n",
    "\n",
    "The methods created for this analysis come w/ tests and docstrings that can be found in the `comics_net.webscraper` and `comics_net.analyzer` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output, display\n",
    "from pandas import DataFrame\n",
    "from PIL import Image\n",
    "\n",
    "from comics_net import analyzer\n",
    "from comics_net.character_map import aliases\n",
    "from comics_net.webscraper import strip_brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the metadata\n",
    "df = analyzer.load_metadata('../metadata/covers.jsonl')\n",
    "\n",
    "# parse issue number and cast as int\n",
    "df['issue_number'] = df['title'].apply(analyzer.get_issue_number_from_title)\n",
    "\n",
    "# describe categorical metadata features\n",
    "df.drop(['issue_number', 'covers'], axis=1).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we show the sumamry statistics of the issue metadata for the comic books scraped thus far. \n",
    "\n",
    "This summary only contains the _issue_ metadata and does not include the _cover_ metadata, which is specific to the image of the comic book cover. Because there are variant covers this information is stored as `dicts` in the \"covers\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Covers & Parse Character Labels\n",
    "\n",
    "Since some issues contain variant covers drawn by different artists and containing different characters on them, each comic book contains issue metadata and cover-image metadata\n",
    "\n",
    "Below we unpack the covers metadata and parse the list of characters appearing on each cover (and variant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cover df\n",
    "covers_dict = df[\"covers\"].to_dict()\n",
    "\n",
    "# add dataframe index to cover image-level metadata\n",
    "for d1 in covers_dict:\n",
    "    for d2 in covers_dict[d1]:\n",
    "        covers_dict[d1][d2][\"index\"] = d1\n",
    "        \n",
    "# unpack cover image-level metadata\n",
    "df_covers = pd.concat([pd.DataFrame(x).T for x in covers_dict.values()], axis=0)\n",
    "\n",
    "# join cover-image-level metadata back with issues metadata\n",
    "df = pd.merge(df.reset_index(),\n",
    "              df_covers,\n",
    "              how='outer',\n",
    "              on='index').\\\n",
    "    sort_values([\"series_name\", \"issue_number\"], ascending=True).\\\n",
    "    drop([\"index\", \"covers\"], axis=1).\\\n",
    "    reset_index(drop=True).\\\n",
    "    copy()\n",
    "\n",
    "# convert cover_characters string to list\n",
    "def map_convert_characters_to_list(characters: str):\n",
    "    if characters is np.nan:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return analyzer.convert_characters_to_list(characters)\n",
    "\n",
    "# listify character string\n",
    "df[\"cover_characters_list\"] = df[\"cover_characters\"].apply(map_convert_characters_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Character Labels\n",
    "\n",
    "The comic book cover cover images and character label data scraped from https://comics.org may contain slight variations in a character label, for example, Superman may be labeled as:\n",
    "```\n",
    "[\n",
    "\"Superman [Clark Kent/ Kal-El]\",\n",
    "\"Superman [Clark Kent]\"\n",
    "\"Superman [Kal-El]\"\n",
    "\"Superman (Earth-1)\"\n",
    "\"Superman (of Earth-1)\"\n",
    "\"Superman (Earth One)\"\n",
    "\"Superman (Earth-2)\"\n",
    "\"Superman (of Earth-2)\"\n",
    "\"Superman [Kal Kent/ Kal-El]\"\n",
    "]\n",
    "```\n",
    "But for the classification and style transfer tasks we will perform, these labels are equivalent and should be treated as one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cover_characters string to list\n",
    "def map_aliases(characters_list: str):\n",
    "    if characters_list is np.nan:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return [aliases.get(character, character) for character in characters_list]\n",
    "    \n",
    "# apply aliases to character names\n",
    "df[\"cover_characters_list_aliases\"] = df[\"cover_characters_list\"].apply(lambda x: map_aliases(x))\n",
    "\n",
    "# drop interim calc. fields\n",
    "df = df.drop(\"cover_characters_list\", axis=1)\n",
    "\n",
    "# where synopisis is blank, replace w/ NaN\n",
    "df.loc[df[\"synopsis\"] == \"\", \"synopsis\"] = np.nan\n",
    "\n",
    "# describe issue and cover metadata\n",
    "df.drop(['issue_number', 'cover_characters_list_aliases'], axis=1).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"cameo\", \"headshot\"/\"head shot\"/\"head only\", \"villain\", \"clone\", \"young\", \"vignette\", \"silhouette\", \"young\", (Earth-1), (Earth-2) from character name???\n",
    "# filter out \"inset\", \"hand only\"/\"only hand\", \"photo\" character names???\n",
    "\n",
    "# explore distribution of substrings in character names\n",
    "all_characters = analyzer.flatten(df[\"cover_characters_list_aliases\"].dropna().values)\n",
    "\n",
    "pd.Series([x for x in all_characters if \"inset\" in x.lower()]).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Team Frequency and Make-up\n",
    "\n",
    "Many comic book characters are part of a larger team and co-occur frequently with their teammates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor/combine this with `map_convert_characters_to_list\" method in analyzer module\n",
    "\n",
    "def map_convert_characters_to_team_list(characters: str):\n",
    "    \"\"\"\n",
    "    Given a character string return the parsed list of unique characters.\n",
    "    \"\"\"\n",
    "    t = analyzer.replace_semicolons_in_brackets(characters)\n",
    "\n",
    "    stack = 0\n",
    "    startIndex = None\n",
    "    results = []\n",
    "\n",
    "    # TODOL: pull this out into it's own function\n",
    "    matches = []\n",
    "    for i, c in enumerate(t):\n",
    "        if c == \"[\":\n",
    "            if stack == 0:\n",
    "                startIndex = i + 1  # string to extract starts one index later\n",
    "\n",
    "            # push to stack\n",
    "            stack += 1\n",
    "        elif c == \"]\":\n",
    "            # pop stack\n",
    "            stack -= 1\n",
    "\n",
    "            if stack == 0:\n",
    "                matches.append((startIndex, i))\n",
    "                results.append(t[startIndex:i])\n",
    "\n",
    "    character_dict: dict = {}\n",
    "    character_dict[\"Teams\"] = {}\n",
    "    character_dict[\"Individuals\"] = {}\n",
    "\n",
    "    for span in matches:\n",
    "        entity = t[span[0] : span[1]]\n",
    "        if entity.count(\";\") == 0:\n",
    "            person_name = analyzer.look_behind(t, span[0])\n",
    "            # apply aliases to character names\n",
    "            person_identity = map_aliases(entity)\n",
    "            character_dict[\"Individuals\"][person_name] = person_identity\n",
    "\n",
    "        elif entity.count(\";\") > 1:\n",
    "            team_name = analyzer.look_behind(t, span[0])\n",
    "            if team_name[:3].lower() == \"the\":\n",
    "                team_name = team_name[3:].strip()\n",
    "            team_members = list(filter(lambda x: x != \"\", entity.split(\"; \")))\n",
    "            character_dict[\"Teams\"][team_name] = map_aliases(team_members)\n",
    "\n",
    "    team_string = analyzer.convert_character_dict_to_str(character_dict)\n",
    "    remainder = analyzer.diff_strings(team_string, t)\n",
    "    remainder = list(filter(lambda x: x != \"\", remainder.split(\"; \")))\n",
    "    character_dict[\"Individuals\"] = map_aliases(remainder)\n",
    "    return character_dict\n",
    "\n",
    "teams = df[\"cover_characters\"].dropna().apply(map_convert_characters_to_team_list)\n",
    "team_list = [x[\"Teams\"] for x in teams if x[\"Teams\"] != {}]\n",
    "\n",
    "team_counts = pd.Series(analyzer.flatten([x.keys() for x in team_list])).value_counts()\n",
    "team_counts[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# team_name = \"Power Pack\"\n",
    "# test = analyzer.flatten([x.get(team_name) for x in team_list if team_name in x.keys()])\n",
    "# pd.Series(collapse_characters(test)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: document what this does,  if we need it?\n",
    "\n",
    "# df_brackets = df[\"cover_characters\"].apply(lambda x: analyzer.match_brackets(str(x)))\n",
    "\n",
    "# df_brackets_unique = pd.Series(analyzer.flatten([list(x.keys()) for x in df_brackets if x != {}])).value_counts().index\n",
    "\n",
    "# df_brackets_unique\n",
    "# [x for x in df_brackets_unique if \";\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Black & White Covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_black_and_white = pd.Series([\"black and white\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_black_white = pd.Series([\"black & white\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_blank = pd.Series([\"blank cover\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_sketch = pd.Series([\"sketch cover\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_wraparound = pd.Series([\"wraparound\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_200_incentive = pd.Series([\"1:200 incentive\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_lenticular = pd.Series([\"lenticular\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_nonlenticular = pd.Series([\"non-lenticular\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_sketch_exclusive = pd.Series([\"sketch exclusive\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "is_photo = pd.Series([\"photo cover\" in x for x in df[\"save_to\"].apply(lambda x: x.lower())])\n",
    "\n",
    "df = df.loc[~(is_black_and_white \n",
    "              | is_black_white \n",
    "              | is_blank \n",
    "              | is_sketch \n",
    "              | is_wraparound \n",
    "              | is_200_incentive \n",
    "              | (is_lenticular & ~is_nonlenticular) \n",
    "              | is_sketch_exclusive\n",
    "              | is_photo), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Distribution of Cover Image Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covers = df[\"save_to\"]\n",
    "\n",
    "# cover_dims = {}\n",
    "# for cover in covers:\n",
    "#     im = Image.open('.' + cover)\n",
    "#     cover_dims[cover] = im.size\n",
    "\n",
    "# covers = pd.DataFrame.from_dict(cover_dims).T.reset_index()\n",
    "# covers.columns = [\"save_to\", \"cover_width\", \"cover_height\"]\n",
    "\n",
    "# \"Calculated dims for {} covers\".format(len(cover_dims.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims = [(x, y) for x, y in zip(covers[\"cover_width\"], covers[\"cover_height\"])]\n",
    "\n",
    "# n = 100\n",
    "# pct = pd.Series(pd.Series(dims).value_counts() / len(dims)).cumsum()[:n].tail(1).values[0] * 100\n",
    "# print('Top {} image sizes account for {}% of covers'.format(n, round(pct, 1)), '\\n')\n",
    "\n",
    "# top_dims = pd.Series(pd.Series(dims).value_counts() / len(dims)).cumsum()[:n].index\n",
    "# top_dims\n",
    "\n",
    "# print('Top Image Sizes:')\n",
    "# print(top_dims.values, '\\n')\n",
    "# print('Height distribution:')\n",
    "# pd.Series(covers[covers[\"cover_width\"] == 400][\"cover_height\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Non-Standard Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_resolution = (400, 613)  # to use when resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(df, covers, on=\"save_to\", how=\"left\")\n",
    "# df = df[df[\"cover_width\"] == 400].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Artist Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove brackets from artists name, listify multiple artists, additional cleanup, etc...\n",
    "def clean_cover_pencils(pencils: str):\n",
    "    if pencils is np.nan:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return pencils.\\\n",
    "    replace(\"(signed)\", \"\").\\\n",
    "    replace(\"(sketch)\", \"\").\\\n",
    "    replace(\"(credited)\", \"\").\\\n",
    "    replace(\"(painted)\", \"\").\\\n",
    "    replace(\"(painting)\", \"\").\\\n",
    "    replace(\"(credited, signed)\", \"\").\\\n",
    "    replace(\"(signed, credited)\", \"\").\\\n",
    "    replace(\"[as GK]\", \"\").\\\n",
    "    strip()\n",
    "    \n",
    "df[\"cover_pencils_cleaned\"] = df[\"cover_pencils\"].apply(clean_cover_pencils)\n",
    "df[\"cover_inks_cleaned\"] = df[\"cover_inks\"].apply(clean_cover_pencils)\n",
    "df[\"cover_colors_cleaned\"] = df[\"cover_colors\"].apply(clean_cover_pencils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Redundant Cover Images\n",
    "\n",
    "Programmatically navigating through deeply linked webpages and scraping assets from a specific site is an imperfect science. Website design changes, pages update, and there are edge cases galore! I did my best to handle edge cases as they occurred but under certain conditions the webscraper pulls down duplicate cover images.\n",
    "\n",
    "For example, if an issue has variant covers such as \"British\" and \"Original\" or \"Direct Sales\" and \"DC Universe\", we may pull  down both. In this case, the cover image is almost identical except for the inset in the barcode area - so technically they are _true_ variants, but it's not a useful distinction for us, so we need to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Base Title from Variant Title Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parse variant description from title\n",
    "df[\"title_variant\"] = df[\"title\"].apply(lambda  x: \" \".join([x.replace(\"[\", \"\").replace(\"]\", \"\") for x in analyzer.match_brackets(x).keys()]))\n",
    "\n",
    "# parse base description from title\n",
    "temp = df[\"title\"].apply(lambda  x: [v[\"start\"] for k, v in analyzer.match_brackets(x).items()]).values\n",
    "df[\"title_base\"] = [t[0][:t[1]].replace(\"[\", \"\").replace(\"]\", \"\").strip() for t in list(zip(df[\"title\"], [x[0] if len(x) > 0 else 0 for x in temp]))]\n",
    "\n",
    "# update base title where no variant\n",
    "df.loc[df[\"title_base\"]==\"\", \"title_base\"] = df.loc[df[\"title_base\"]==\"\", \"title\"]\n",
    "\n",
    "# count instances of base title descriptions\n",
    "title_base_counts = df[\"title_base\"].value_counts().reset_index()\n",
    "title_base_counts.columns = [\"title_base\", \"title_base_count\"]\n",
    "\n",
    "# merge count of base titles\n",
    "df = pd.merge(df, title_base_counts, how=\"left\", on=\"title_base\").sort_values(\"title_base_count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is parse the _base_ and _variant_ names from the \"title\" field and counted of the number of _base_ titles. This gives us a foundation to reason about variants and begin the process of deduping ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"title\", \"title_base\", \"title_variant\", \"title_base_count\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the number of _base_ titles is greater than 1, there may be 1) a dupe, 2) a variant, or 3) a re-numbering of the series. These are the cases we need to handle while deduping, and we only want to remove instances of a dupe while keeping any variants or re-numberings.\n",
    "\n",
    "If the number of _base_ titles is 1, then we can rest assured there is no dupe.\n",
    "\n",
    "#### Filter for Non-Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only those issues not having variants / dupes\n",
    "df_non_variants = df[(df[\"title_base_count\"] == 1)].sort_values([\"title\", \"issue_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for Variants (and Possible Dupes)\n",
    "\n",
    "Any issue containing more than 1 instance of base title is considered a variant or possible dupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only those issues having variants / dupes\n",
    "df_variants = df[df[\"title_base_count\"] > 1].sort_values([\"title\", \"issue_number\"])\n",
    "\n",
    "\"Of {} total covers, {} are either variants or redundant and {} are unique\".format(len(df_variants) + len(df_non_variants), len(df_variants), len(df_non_variants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping `df_variants` by `title_base` and `cover_pencils`, we can find which issue variants where there is more than one artist per issue. If there is more than one artist we assume it is a true variant and not a duplicate cover image.\n",
    "\n",
    "#### Filter for True Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count pencil artists by base title\n",
    "df_variants_artists = df_variants.groupby([\"title_base\", \"cover_pencils_cleaned\"]).apply(len).reset_index()\n",
    "df_variants_artists.columns = [\"title_base\", \"cover_pencils_cleaned\", \"base_pencils_count\"]\n",
    "\n",
    "# merge count of pencil artists by base title with df\n",
    "df_variants = pd.merge(df_variants, df_variants_artists, how=\"left\", on=[\"title_base\", \"cover_pencils_cleaned\"])\n",
    "\n",
    "# if the base title / pencil artist combination equals 1, it's a true variant and not dupe\n",
    "non_dupes = df_variants[df_variants[\"base_pencils_count\"] == 1.0].drop(\"base_pencils_count\", axis=1)\n",
    "\n",
    "# concat non-dupes with non-variants\n",
    "df_non_dupes = pd.concat([df_non_variants, non_dupes], axis=0)\n",
    "\n",
    "\"{} covers have been identified as non-dupes\".format(len(df_non_dupes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for Possible Dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_dupes = df_variants[df_variants[\"base_pencils_count\"] > 1.0].drop(\"base_pencils_count\", axis=1)\n",
    "\n",
    "\"{} covers have been identified as possible dupes\".format(len(possible_dupes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find True Dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_likeness(path_a: str,  path_b: str):\n",
    "    h1 = Image.open('.' + path_a).convert('L').resize((400, 613)).histogram()\n",
    "    h2 = Image.open('.' + path_b).convert('L').resize((400, 613)).histogram()\n",
    "\n",
    "    return round(math.sqrt(reduce(operator.add, map(lambda a,b: (a-b)**2, h1, h2))/len(h1)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a = df_non_dupes.reset_index()[\"save_to\"][10]\n",
    "path_b = df_non_dupes.reset_index()[\"save_to\"][11]\n",
    "\n",
    "display(Image.open('.' + path_a).resize((150, 225)), Image.open('.' + path_b).resize((150, 225)))\n",
    "\n",
    "print(\"Image similarity of {} and {} is {}\".format(path_a, path_b, image_likeness(path_a,  path_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant files from possible_dupes  (for some reason, some are counted more than once)\n",
    "# TODO: find where these dupes are coming from!!!!\n",
    "possible_dupes = possible_dupes.loc[possible_dupes.astype(str).drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "n = len(possible_dupes['title_base'].unique())\n",
    "\n",
    "counter = 0\n",
    "likeness = {}\n",
    "for title in possible_dupes[\"title_base\"].unique():\n",
    "    clear_output()\n",
    "    counter += 1\n",
    "    print(\"{} - {} of {}\".format(title, counter, n))\n",
    "    likeness[title] = {}\n",
    "    tf = possible_dupes[possible_dupes[\"title_base\"] == title].reset_index(drop=True)[\"save_to\"]\n",
    "    if len(tf) == 1:\n",
    "        img = tf.loc[0]\n",
    "        likeness[title][img] = {}\n",
    "        likeness[title][img][\"similarity\"] = np.nan\n",
    "    else:\n",
    "        for i in range(0, len(tf)):\n",
    "            if i == len(tf)-1:\n",
    "                pass\n",
    "            else:\n",
    "                img = tf.loc[i]\n",
    "                likeness[title][img] = {}\n",
    "                likeness[title][img][\"similarity\"] = min([image_likeness(img, x) for x in tf.loc[i+1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []\n",
    "for k in likeness:\n",
    "    try:\n",
    "        similarity.append([(k, list(v.values())[0]) for k,v in likeness[k].items()])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "dupes = pd.DataFrame(analyzer.flatten(similarity), \n",
    "                     columns=[\"save_to\", \"within_series_likeness\"]).\\\n",
    "    sort_values(\"within_series_likeness\").\\\n",
    "    reset_index(drop=True)\n",
    "\n",
    "# merge possible dupes and dupes\n",
    "possible_dupes = pd.merge(possible_dupes, dupes, on=\"save_to\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "idx+=1\n",
    "thingy = possible_dupes[\"title_base\"].unique()[idx]\n",
    "\n",
    "temp = possible_dupes[(possible_dupes[\"title_base\"] == thingy)][[\"save_to\", \"within_series_likeness\"]]\n",
    "temp\n",
    "\n",
    "print(idx)\n",
    "print(thingy, '\\n')\n",
    "for i in range(0, len(temp)):\n",
    "    path = temp[\"save_to\"].iloc[i]\n",
    "    display(Image.open('.' + path).resize((150, 225)))\n",
    "    print(\"{} = {}\".format(i, temp[\"within_series_likeness\"].iloc[i]))\n",
    "    if temp[\"within_series_likeness\"].iloc[i] is np.nan:\n",
    "        print(\"keep\")\n",
    "    elif temp[\"within_series_likeness\"].iloc[i] <= 700:\n",
    "        print(\"discard\")\n",
    "    else:\n",
    "        print(\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(image_likeness(temp.iloc[0][\"save_to\"], temp.iloc[1][\"save_to\"]))\n",
    "# temp[\"save_to\"].values\n",
    "\n",
    "# possible_dupes[possible_dupes[\"title_base\"] == thingy].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Dupes & Combine w/ `df_non_dupes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules to apply when deduping possible dupes\n",
    "# 1) always include within likeness == 'nan'\n",
    "# 2) exclude where likeness <= 500\n",
    "\n",
    "is_null = possible_dupes[\"within_series_likeness\"].isnull()\n",
    "not_similar = possible_dupes[\"within_series_likeness\"] > 700\n",
    "\n",
    "keep = possible_dupes[(is_null | not_similar)]\n",
    "df_non_dupes = pd.concat([df_non_dupes, keep[df_non_dupes.columns]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{} covers have been identified as suitable for our machine learning tasks\".format(len(df_non_dupes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a random comic book cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample_of_covers(df: DataFrame, character: str, n: int) -> dict:\n",
    "    \"\"\"\n",
    "    Given a DataFrame of covers (TODO: specify that schema) and a character return a\n",
    "    dict (TODO: specify that schema) of n randomly sampled covers for that character.\n",
    "    \"\"\"\n",
    "    df = df[df[\"cover_characters_list_aliases\"].apply(lambda x: character in x)].reset_index(drop=True)\n",
    "\n",
    "    s = list(range(0, max(df.index)))\n",
    "    random.shuffle(s)\n",
    "\n",
    "    covers: dict = dict()\n",
    "    for i in s[:n]:\n",
    "        covers[\"cover_{}\".format(i)] = {}\n",
    "        covers[\"cover_{}\".format(i)][\"synopsis\"] = df[\"synopsis\"].iloc[i]\n",
    "        covers[\"cover_{}\".format(i)][\"image_path\"] = df[\"save_to\"].iloc[i]\n",
    "        covers[\"cover_{}\".format(i)][\"cover_inks\"] = df[\"cover_inks\"].iloc[i]\n",
    "        covers[\"cover_{}\".format(i)][\"cover_colors\"] = df[\"cover_colors\"].iloc[i]\n",
    "        covers[\"cover_{}\".format(i)][\"cover_pencils\"] = df[\"cover_pencils_cleaned\"].iloc[i]\n",
    "        covers[\"cover_{}\".format(i)][\"characters\"] = df[\"cover_characters_list_aliases\"].iloc[i]\n",
    "\n",
    "    return covers\n",
    "\n",
    "random_cover = get_random_sample_of_covers(df_non_dupes[df_non_dupes[\"cover_characters_list_aliases\"].notnull()], \n",
    "                                                    character=\"Archie Andrews\", \n",
    "                                                    n=1)\n",
    "\n",
    "synopsis = random_cover[list(random_cover.keys())[0]][\"synopsis\"]\n",
    "cover_pencils = random_cover[list(random_cover.keys())[0]][\"cover_pencils\"]\n",
    "cover_inks = random_cover[list(random_cover.keys())[0]][\"cover_inks\"]\n",
    "cover_colors = random_cover[list(random_cover.keys())[0]][\"cover_colors\"]\n",
    "characters = random_cover[list(random_cover.keys())[0]][\"characters\"]\n",
    "\n",
    "\n",
    "print(\"Pencils: {}, Colors: {}, Colors: {}\".format(cover_pencils, cover_inks, cover_colors), '\\n')\n",
    "\n",
    "print(\"Characters: {}\".format(characters), '\\n')\n",
    "print(\"Synopsis: {}\".format(synopsis), '\\n')\n",
    "\n",
    "image_path = random_cover[list(random_cover.keys())[0]][\"image_path\"]\n",
    "im = Image.open(\".\" + image_path)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Specific Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_characters = pd.Series(analyzer.flatten(df[\"cover_characters_list_aliases\"].dropna().values)).value_counts()\n",
    "unique_characters.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_team_members(team_name, pct):\n",
    "    cumsum_teams = pd.Series(pd.Series([str(x) for x in team_list if team_name in list(x.keys())]).value_counts() / team_counts[team_name]).cumsum()\n",
    "    top = [ast.literal_eval(x) for x in cumsum_teams[cumsum_teams < pct].index]\n",
    "\n",
    "    top_dict = dict()\n",
    "    top_dict[team_name] = set()\n",
    "    for d in top:\n",
    "        top_dict[team_name] = top_dict[team_name] | set(list(d.values())[0])\n",
    "\n",
    "    return top_dict[team_name]\n",
    "\n",
    "# explore team makeup\n",
    "pct = 50\n",
    "team = get_team_members(\"Defenders\", pct=pct/100)\n",
    "\n",
    "print(\"{} appears {} times\".format(\"Defenders\", team_counts[\"Defenders\"], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_params = {\n",
    "    \"Fantastic Four\": {\"pct\": 0.50},\n",
    "    \"X-Men\": {\"pct\": 0.35},\n",
    "    \"Avengers\": {\"pct\": 0.20},\n",
    "    \"Justice League of America\": {\"pct\": 0.25},\n",
    "    \"Legion of Super-Heroes\": {\"pct\": 0.15},\n",
    "    \"Teen Titans\": {\"pct\": 0.20},\n",
    "    \"Defenders\": {\"pct\": 0.50},\n",
    "    \"Justice Society of America\": {\"pct\": 0.10},\n",
    "    \"Justice League\": {\"pct\": 0.10},\n",
    "    \"Guardians of the Galaxy\": {\"pct\": 0.10},\n",
    "    \"X-Factor\": {\"pct\": 0.20},\n",
    "    \"Outsiders\": {\"pct\": 0.10},\n",
    "    \"Alpha Flight\": {\"pct\": 0.10},\n",
    "    \"Suicide Squad\": {\"pct\": 0.20},\n",
    "    \"Inhumans\": {\"pct\": 0.45},\n",
    "    \"Doom Patrol\": {\"pct\": 0.75},\n",
    "    \"Archies\": {\"pct\": 0.80},\n",
    "    \"Green Lantern Corps\": {\"pct\": 0.25},\n",
    "    \"Excalibur\": {\"pct\": 0.25},\n",
    "    \"X-Force\": {\"pct\": 0.20}\n",
    "}\n",
    "\n",
    "for team in team_params.keys():\n",
    "    team_params[team][\"members\"] = get_team_members(team, team_params[team][\"pct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_params[\"X-Men\"][\"members\"] = team_params[\"X-Men\"][\"members\"] | team_params[\"X-Factor\"][\"members\"]\n",
    "team_params[\"X-Men\"][\"members\"] = team_params[\"X-Men\"][\"members\"] | team_params[\"X-Force\"][\"members\"]\n",
    "team_params[\"X-Men\"][\"members\"] = team_params[\"X-Men\"][\"members\"] | team_params[\"Excalibur\"][\"members\"]\n",
    "team_params.pop(\"X-Factor\")\n",
    "team_params.pop(\"X-Force\")\n",
    "team_params.pop(\"Excalibur\")\n",
    "\n",
    "team_params[\"Justice League\"][\"members\"] = team_params[\"Justice League\"][\"members\"] | team_params[\"Justice League of America\"][\"members\"]\n",
    "team_params[\"Justice League\"][\"members\"] = team_params[\"Justice League\"][\"members\"] | team_params[\"Justice Society of America\"][\"members\"]\n",
    "team_params[\"Justice League\"][\"members\"] = team_params[\"Justice League\"][\"members\"] | team_params[\"Teen Titans\"][\"members\"]\n",
    "team_params.pop(\"Justice League of America\")\n",
    "team_params.pop(\"Justice Society of America\")\n",
    "team_params.pop(\"Teen Titans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in team_params.keys():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_concurrence(df: DataFrame, character: str, character_counts: dict, top_n: int, greater_than:int = 100, found=set()):\n",
    "    df = df[\"cover_characters_list_aliases\"].dropna()\n",
    "    df = df[df.apply(lambda x: character in x)]\n",
    "    counts = pd.Series(analyzer.flatten(df.values)).value_counts()\n",
    "    characters = counts[counts >= greater_than]\n",
    "    characters = characters.index\n",
    "    characters = [x for x in characters if x not in found]\n",
    "    characters = list(filter(lambda x: (x!= \"''\") & (x != \"'\") & (x !=','), characters))\n",
    "    characters = characters[:top_n]\n",
    "    return {k: character_counts[k] for k in characters}\n",
    "\n",
    "top_n = 20\n",
    "greater_than = 100\n",
    "\n",
    "found = set()\n",
    "for team in team_params.keys():\n",
    "    characters_dict = {}\n",
    "    if team == \"Fantastic Four\":\n",
    "        top_n = 1\n",
    "    for member in team_params[team][\"members\"]:\n",
    "        if member not in found:\n",
    "            d = character_concurrence(df=df_non_dupes, character=member, character_counts=unique_characters, top_n=top_n, greater_than=greater_than, found=found)\n",
    "            characters_dict.update(d)\n",
    "    found = found | set(characters_dict.keys())\n",
    "    team_params[team][\"final_roster\"] = characters_dict\n",
    "\n",
    "# team_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Dirs\n",
    "\n",
    "#### Cover Images\n",
    "\n",
    "There are a total of 1,281,167 images for training. The number of images for each synset (category) ranges from 732 to 1300. There are 50,000 validation images, with 50 images per synset. There are 100,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [250, 500, 750, 1000, 2000, 4000]\n",
    "\n",
    "for i in counts:\n",
    "    team_params[\"characters_more_than_{}\".format(i)] = {}\n",
    "    team_params[\"characters_more_than_{}\".format(i)][\"final_roster\"] = {k: v for k,v in zip(unique_characters.index, unique_characters[unique_characters >= i])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in counts:\n",
    "#     print(n)\n",
    "#     analyzer.create_training_dirs(df_non_dupes[df_non_dupes[\"cover_characters_list_aliases\"].notnull()], \n",
    "#                                       characters_dict=team_params[\"characters_more_than_{}\".format(n)][\"final_roster\"],\n",
    "#                                       save_dir=\"characters_more_than_{}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: remove any image files that contain a \"\\t\", since this is the column delimiter used by our DataBunch loader\n",
    "# # TODO: create a test/ dir\n",
    "\n",
    "# for team in team_params:\n",
    "#     print(team)\n",
    "#     save_dir = team.replace(\" \", \"_\").lower()\n",
    "#     analyzer.create_training_dirs(df_non_dupes[df_non_dupes[\"cover_characters_list_aliases\"].notnull()], \n",
    "#                                   characters_dict=team_params[team][\"final_roster\"],\n",
    "#                                   save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in team_params:\n",
    "#     print(\"https://comics-net.s3-us-west-1.amazonaws.com/{}.tgz\".format(d.replace(\" \", \"_\").lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Dirs\n",
    "\n",
    "#### Cover Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write synopses to dir\n",
    "\n",
    "ordered_synopsis = df_non_dupes[df_non_dupes[\"synopsis\"].\\\n",
    "                     notnull()][[\"series_name\", \"on_sale_year\",  \"issue_number\", \"synopsis\",  \"cover_genre\"]].\\\n",
    "                     sort_values([\"series_name\", \"on_sale_year\",  \"issue_number\"], ascending=True).\\\n",
    "                     reset_index(drop=True)\n",
    "\n",
    "\n",
    "ordered_synopsis[\"on_sale_year\"] = ordered_synopsis[\"on_sale_year\"].fillna(9999.0).astype(int).astype(str)\n",
    "\n",
    "ordered_synopsis = ordered_synopsis[ordered_synopsis[\"synopsis\"].notnull()].drop_duplicates()\n",
    "\n",
    "genre_by_series = ordered_synopsis.groupby([\"series_name\",\"cover_genre\"]).\\\n",
    "                    apply(len).\\\n",
    "                    reset_index().\\\n",
    "                    dropna().\\\n",
    "                    groupby([\"series_name\"]).apply(max).\\\n",
    "                    reset_index(drop=True).\\\n",
    "                    drop(0, axis=1)\n",
    "\n",
    "genres = pd.merge(ordered_synopsis, genre_by_series, on=\"series_name\", how=\"left\")[[\"cover_genre_x\", \"cover_genre_y\"]].values\n",
    "genres = [x[0] if x[0] is not np.nan else x[1] for x in genres]\n",
    "\n",
    "ordered_synopsis.drop(\"cover_genre\", axis=1)\n",
    "ordered_synopsis[\"cover_genre\"] = genres\n",
    "ordered_synopsis[\"cover_genre\"] = ordered_synopsis[\"cover_genre\"].fillna(\"NaN\")\n",
    "\n",
    "ordered_synopsis[\"cover_genre\"] = ordered_synopsis[\"cover_genre\"].apply(lambda x: x.strip()).\\\n",
    "    apply(lambda x: x.replace(\"super-h√©roes (superhero)\", \"superhero\")).\\\n",
    "    apply(lambda x: x.replace(\"anthropomorphic-funny animals\", \"animals\"))\n",
    "\n",
    "genres = set(analyzer.flatten(ordered_synopsis[\"cover_genre\"].apply(lambda x: analyzer.convert_characters_to_list(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genres:\n",
    "    ordered_synopsis[genre] = ordered_synopsis[\"cover_genre\"].apply(lambda x: int(genre in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory of unsupervised text\n",
    "for genre in genres:\n",
    "    df_genre = ordered_synopsis[ordered_synopsis[genre] == 1][[\"synopsis\"]]\n",
    "    df_genre.columns = [\"text\"]\n",
    "    os.mkdir(\"./text/unsup/{}\".format(genre.replace(\"; \", \"_\")))\n",
    "    df_genre.to_csv(\"./text/unsup/{}/synopsis.csv\".format(genre.replace(\"; \", \"_\")), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory of supervised text\n",
    "shuffled_synopsis = ordered_synopsis.sample(frac=1).reset_index(drop=True)[list(genres) + [\"synopsis\"]]\n",
    "\n",
    "idx = int(round(len(shuffled_synopsis) * 0.80, 0))\n",
    "\n",
    "train_synopsis = shuffled_synopsis[:idx]\n",
    "test_synopsis = shuffled_synopsis[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_synopsis.to_csv(\"./text/train/synopsis.csv\".format(genre.replace(\"; \", \"_\")), index=False)\n",
    "test_synopsis.to_csv(\"./text/test/synopsis.csv\".format(genre.replace(\"; \", \"_\")), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: load metadata and return aggregate / summary statistics\n",
    "\n",
    "logged_metadata = []\n",
    "\n",
    "with jsonlines.open('../metadata/log.jsonl', mode='r') as reader:\n",
    "    for item in reader:\n",
    "        logged_metadata.append(item)\n",
    "        \n",
    "log = pd.DataFrame(logged_metadata)\n",
    "\n",
    "log[[\"issue_count\", \"publisher_id\", \"publisher_page\"]] = log[[\"issue_count\", \"publisher_id\", \"publisher_page\"]].astype(int)\n",
    "\n",
    "sorted_log = log.\\\n",
    "drop_duplicates().\\\n",
    "sort_values(by=[\"publisher_id\",  \"publisher_page\", \"series\"])\\\n",
    "[[\"publisher_id\",  \"publisher_page\", \"series\", \"issue_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_log[sorted_log[\"publisher_id\"] == 78][75:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comics-net (env)",
   "language": "python",
   "name": "comics-net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
